<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Forecasting Economic Growth Using Business News Data</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="./bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="./css/style.css" rel="stylesheet">
    <style>
        body {
        padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
        }
    </style>
    <link href="./bootstrap/css/bootstrap-responsive.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
    <script src="../bootstrap/js/html5shiv.js"></script>
    <![endif]-->

    <!-- Fav icons -->
    <link rel="shortcut icon" href="./images/harvard.png">

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <!--<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>

    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./bootstrap/js/bootstrap.min.js"></script>
</head>

<body>


<div class="navbar navbar-inverse navbar-fixed-top">
    <div class="navbar-inner">
        <div class="container">
            <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="brand" href="#">CS109a-Project</a>
            <div class="nav-collapse collapse">
                <ul class="nav">
                    <li class="active"><a href="#home">Home</a></li>
                    <li><a href="#ipynb">Analysis</a></li>
                    <li><a href="#references">References</a></li>
                    <li><a href="#about">About us</a></li>
                </ul>
                <a id="forkme_banner" href="https://github.com/dkmalav/cs109a-project">View on GitHub</a>
            </div><!--/.nav-collapse -->
        </div>
    </div>
</div>

<div id="home" class="container content">
    <!--<div class="hero-unit" style="background: url('http://nite-lite.org/wp-content/uploads/2016/06/business-news.jpg')">-->
    <div class="hero-unit" style="background-color: #2ba6cb; text-align: center;">
        <h1><font color="white">Forecasting Economic Growth Using Business News</font></h1>
        <div><font color="black">Dinesh Malav, Maja Campara, Naveen Sinha, Udaykiran Tadiparty</font></div>
    </div>
    <div>
        <h3>Introduction</h3>
        <hr/>
        <div>
            <p>
                The power of the media has always proven to be an undeniable force in influencing its sentiment and
                projecting trends and analysis on us. More specifically, in the world of business news, there is a wide
                array of sentiment analyzing economic growth and influencing readers’ spending. We therefore would like
                to assess the predictive value in the sentiment that is contained in business news. The impact of news
                headlines has previously been studied providing frameworks for using news articles
                and economic data to model real world economic problems. For example, models based on macroeconomic
                indicators can use current economic news sentiment as a better indicator of market movement where, short
                to middle term behavior can be better modeled using current news.
            </p>
            <p>
                As there are many different approaches possible in tackling this project scope along with powerful
                resources in text mining at our disposal, sentiment analysis will be the key factor in analyzing
                business news articles while predictive modelling tools such as time series will be used for forecasting
                economic growth based on the sentiments we are able to identify.
            </p>
        </div>

        <h3>Objective</h3>
        <hr/>
        <div>
            Predict GDP (economic growth) from the sentiment that is contained in
            <a href="http://www.nytimes.com/">The New York Times</a> business news,
            perhaps supplemented with traditional economic indicators.
        </div>

        <h3>Data Collection</h3>
        <hr/>

        <h4>Economic Data Collection</h4>

        <div>
            To analyze and assess GDP data for the U.S., quarterly and yearly data were downloaded from
            <a href="http://www.bea.gov/">U.S. Bureau of Economic Analysis (BEA)</a>
            dating back to 1947. This data contains GDP value in current dollars and chained
            2009 dollars in billions and is released on a quarterly basis of the last business day of thenext quarter.
            Current Dollar GDP involves calculating economic activity in present-day dollars however, this makes time
            period comparisons difficult due to the effects of inflation. Alternatively, Constant Dollar GDP factors out
            the impact of inflation and allows for easy comparisons by converting the value of the dollar in other time
            periods to present-day dollars. Therefore, we consider the rate of change in GDP chained 2009 dollars to GDP
            to assess growth and recession more accurately over time. The following two charts show the GDP and the rate
            of change in GDP value compared to previous quarters. It was observed that rate of change does not follow
            any particular pattern and as per our hypothesis, the news sentiment for the same period should directly
            correlate with the rate of change in GDP.
        </div>

        <div class="row">
            <div class="span6" style="text-align: center;">
                <img src="./images/usgdp.png">
            </div>
            <div class="span6" style="text-align: center;">
                <img src="./images/usgdp_rate.png">
            </div>
        </div>

        <h4>Business News Collection</h4>

        <div>
            <p>
                In order to establish correlation between business news and economic growth, it is important to identify
                relevant news and classify them into 'good' or 'bad' news categories. This section describes how the
                business news was collected and converted into features for training machine learning models.
            </p>

            <p>
                The New York Times (NYTimes) provides an Article Search API to fetch data related to news articles. This
                API does not
                return full text of articles, but it returns a number of helpful metadata such as subject terms,
                abstracts,
                lead paragraphs, and dates, as well as URLs which one could conceivably use to scrape the full text of
                articles. For this project only lead paragraphs for business articles for specified time periods were
                collected. In order to request data from the NYTimes, an API key must be requested from the
                <a href="http://developer.nytimes.com/">NYT Developers Network</a>. The article search API is
                limited to 1000 calls per day, and 5 calls per second. These
                limits imposed challenges for collecting as much data as possible for a day and time period hence, the
                data
                collection was limited to two API calls or 20 articles per day. This allowed us to request 500 days’
                worth
                of data per day. A python wrapper was developed around the NYTimes Article Search API to get articles
                for a
                given start and end date. The reqested lead paragraph data were then stored in a file with the file name
                reflecting the day in the format YYYYMMDD.txt
            </p>

        </div>
        <div>
            The following code wraps NYTimes API in a python class. This code was used to periodically run every day to make the number of allowed calls to the NYTimes API.
        </div>
        <pre class="pre-scrollable">
import os
import requests
import time
import json
from datetime import datetime, timedelta


class NYTArticlesApi(object):
    def __init__(self, api_key):
        self.api_key = api_key
        self.url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'

    def get_articles(self, start_date, end_date, page):
        start_date_str = start_date.strftime('%Y%m%d')
        end_date_str = end_date.strftime('%Y%m%d')

        params = {
            'api-key': self.api_key,
            'fq': "section_name:Business",
            'begin_date': start_date_str,
            'end_date': end_date_str,
            'fl': "lead_paragraph",
            'sort': "newest",
            'page': page
        }
        r = requests.get(self.url, params=params)

        if r.status_code == 200:
            response = json.loads(r.text)
            docs = response['response']['docs']
            return [item['lead_paragraph'] for item in docs]

        print "Error: api response: ", r.status_code, "Message :", r.text
        return None

    def fetch_articles(self, start_date, end_date, data_dir):
        cur_date = start_date
        while cur_date >= end_date:
            print 'Processing ', cur_date, '...'
            articles = []
            # get 2 pages for each day, i.e. 20 articles
            for page in range(2):
                articles_page = self.get_articles(cur_date, cur_date, page)
                if articles_page:
                    articles += articles_page
                else:
                    print "No data for page {0}.".format(page)
                    time.sleep(1)
                    break
                time.sleep(1)

            if len(articles) > 0:
                file_name = data_dir + cur_date.strftime('%Y%m%d') + '.txt'
                with open(file_name, 'wb',) as f:
                    for text in articles:
                        if text:
                            f.write(text.encode('utf-8').strip() + '\n')

            else:
                print "Error fetching articles for {0}.".format(cur_date)
                file_name = data_dir + "last_date.txt"
                with open(file_name, 'wb',) as f:
                    f.write(cur_date.strftime('%Y%m%d'))
                return

            cur_date -= timedelta(days=1)
        </pre>

        <div>
            On average around 7000 articles were collected for one year. It was discovered by reading some articles that
            not all the articles were US news, these articles can be filtered by checking if any of the country name
            other than US was present. It would also allow for testing if rest of the world news also affects the
            economic grown in US.
        </div>

        <div>
            The articles for each day were saved to individual files however, it was difficult to manage this data. A
            wrapper class was then developed for saving data to a SQLite database for easier management and querying
            data as required.
        </div>


        <pre class="pre-scrollable">
# Class to load data in a SQLite database
class SqliteDB(object):
    def __init__(self, db_file):
        self.db_file = db_file
        self.con = sqlite.connect(db_file)
        self.cur = self.con.cursor()

    def close_connection(self):
        self.con.close()

    def create_articles_table(self):
        self.cur.execute("CREATE TABLE Articles(Day DATE, ArticleId INT, Article TEXT, UNIQUE(Day, ArticleId) ON CONFLICT REPLACE)")

    def insert_articles(self, file_name, cur_date):
        with open(file_name, 'rb',) as f:
            articles = f.readlines()

        for i, article in enumerate(articles):
            article = article.replace('\'', '')
            self.cur.execute("INSERT INTO Articles VALUES('{0}','{1}','{2}')".format(cur_date, i, article))


    def insert_data(self, data_dir, start_date, end_date):
        print start_date, end_date
        cur_date = start_date
        while cur_date >= end_date:
            cur_date_str = cur_date.strftime('%Y-%m-%d')
            print cur_date_str
            file_name = data_dir + cur_date.strftime('%Y%m%d') + '.txt'
            # print file_name
            if os.path.exists(file_name):
                self.insert_articles(file_name, cur_date_str)

            cur_date -= timedelta(days=1)

        self.con.commit()

    def query_data(self, query):
        df = pd.read_sql(query, self.con)
        return df;
        </pre>

        <div>
            Once the data is loaded into the database, it can be queried to easily group data for any given time period.
        </div>
        <pre class="pre-scrollable">
ddb = SqliteDB('../data/articles.sqlite')
query = "SELECT strftime(\"%Y\", Day) as 'year', count(*) as 'count' FROM Articles group by strftime(\"%Y\", Day)"
df = db.query_data(query)

f, axs = plt.subplots(1,1,figsize=(15,5))
plt.title('Number of articles vs year')
plt.ylabel('Number of articles')
df.plot(kind='bar', x='year', y='count', ax=axs, alpha=0.5)
plt.show()
        </pre>

        <div class="row">
            <div class="span12" style="text-align: center;">
                <img src="./images/article_count.png">
            </div>
        </div>

        <h3>Text Mining for Sentiment Scores</h3>
        <hr/>
        <div>
            One of the important aspects of determining the impact of the business on the GDP was to extract information
            from the words and sentences we were looking at. The way we tried to do that was through sentiment analysis.
            In order to perform sentiment Analysis we used the following approach :
            <ul>
                <li>
                    Since the GDP data was available quarterly, we combined the New York Times data that we received daily to
                    build a corpus for each quarter.
                </li>
                <li>
                    Using the NLTK Sentence Tokenizer, we split the quarterly corpus of into sentences and further into words
                    using the Word Tokenizer
                </li>
                <li>
                    For every word,
                    <ul>
                        <li>
                            We determined the part of speech it belonged to. Only if the word was one of Noun, Adjective, Verb or Adverb
                            was it considered for sentiment scoring
                        </li>
                        <li>
                            Word sense disambiguation was perform on each word to determine the sense of each word in the sentence using
                            NLTK’s less algorithm
                        </li>
                        <li>
                            The output of the above step was a synset composed of the word sense and the part of speech. This synset was
                            then passed to the SentiWord net to get the sentiment score for each word.
                        </li>

                    </ul>
                </li>
                <li>
                    The sentiment score, both positive and negative, received in the above fashion was then aggregated per
                    quarter to determine the overall positive and negative sentiment of the NY Times corpus for that quarter.
                    These sentiment scores then act as exogenous variables when we run the ARIMA model on the endogenous GDP
                    rate.
                </li>
            </ul>
        </div>

        <h3>Data Modeling and Analysis</h3>
        <hr/>
        <div>
            <h4>Assessing the data and criteria for best model selection</h4>

            <div>
                As we are trying to forecast U.S. GDP using business news sentiments, we had determined that this can be
                done using a time series model. This is different from a regular regression problem since: 1) the
                problem is time dependent and 2) the data in most cases experiences some form of trend over time,
                usually seasonality trends.
            </div>
            <div>
                The data analysis performed assesses different models and compares them on six varying crietrion:
                forecast error, root mean square error, AIC, BIC, Dickey-Fuller test and the Durbin-Watson test. Here we
                would like to see the forecast error and root mean square error to be as close to zero, while the AIC
                and BIC should be as minimized as can be. The Dickey-Fuller test does a stationarity check to confirm if
                trends are still in the data and the Durbin-Watson test indicates whether we have auto-correlation. A
                comparison of these results across all the models are in the table below and discussed further below as
                well.
            </div>
            <div>
                The data used as we know is U.S. GDP however, for the modelling we used the rate of change of GDP
                instead. Along with the rate, we also read in the positive and negative scores as the predictors and
                time as a numeric indicator.
            </div>
            <h4>Basic Models: Mean Model, Linear Trend and Baseline (AR(1)) Model</h4>

            <div>
                The first three most basic models implemented were: the Mean Constant Model, Linear Trend Model, and
                Random Walk Model. From these three, the random walk model which is our baseline model outperformed the
                other two. This seems obvious because a mean constant model is a horizontal line at the value of the
                mean rate while a linear regression would not be ideal since it assumes observations are independent
                which is not the case for a time series problem. The chosen baseline model is an auto regressive model
                with a p-lag of one. This is also called a random walk problem where the series is equally likely to go
                up or down in the next period regardless of what it has done in the past. Looking at the criteria
                comparison in the table, the forecast error and the RMSE are close to each other as well as the AIC and
                BIC for the linear trend and baseline model however, the Durbin-Watson test shows that the baseline is a
                better model because its value is close to 2 (very little auto-correlation).
            </div>
            <h4>Stationarity, Smoothing, Decomposition and DIfferencing</h4>

            <div>
                Once the baseline model was established, to implement other time series models we needed the data to be
                stationary. The underlying principle is to model or estimate the trend and seasonality in the series and
                remove those from the series to get a stationary series. Two models were looked at for estimating and
                eliminating trend: a simple moving average model and a simple exponential smoothing model. These models
                used short-term averaging which tends to have the effect of smoothing out the bumps in the original
                series. With this, we wanted to see if an optimal balance between the performance of the mean and random
                walk models could be seen. Based on the six criteria, we were able to derive the forecast error, RMSE
                and Dickey-Fuller which shows that both perform just as well to each other and are stationary however,
                they do not necessarily perform better than the baseline model.
            </div>
            <div>
                We then tried using the approch of differencing and decomposition to see if we can improve the series.
                Decomposition models trend and seasonality and then removes it from the series. The results outputted
                the smallest forecast error and RMSE yet and passed the Dickey-Fuller test for stationarity as well. For
                the remaining three models, first order differencing was used to remove trend along with the sentiment
                score predictors as well which are used in the models.
            </div>
            <h4>Autoregressive and Moving Average Models</h4>

            <div>
                The next two models examined are the auto regressive and moving average models. We wanted to see if we
                can improve from the last models implemented by using a first order difference, inputting the predictors
                and increasing the lag parameter to 2. The auto regressive model seems to outperform the rest so far as
                its Durbin-Watson test is the closest to 2 (no autocorrelation). However, the moving average model had
                the lowest AIC and BIC values while both of these models had slightly higher forecast errors and RMSE
                than the baseline model. The two models perform very closely to each other overall however, our next
                steps involved trying to optimize the lag parameters to assess our final (ARIMA) models and perform
                cross validation.
            </div>
            <h4>Cross Validation</h4>

            <div>
                In order to perform cross validation on the dataset using ARIMA, we had to make sure that we split a
                dataset into test and train in such a way that the training set maintained similar correlation as
                identified on the entire dataset using the ACF and PACF. The AR or MA models did not converge if the
                above condition was not satisfied. Since the ARIMA model highly depends on the correlation between the
                time series data, we could not do a K-FOLD cross validation as there is no guarantee that the time
                periods necessary for prediction would be available in the fold. Hence in order to perform cross
                validation, we split the data such that for example, for predicting 81st period, the first 80 time
                periods were used for training the model and similarly for predicting the 82nd time period, the first 81
                time periods were used and so on. Root mean square error, Mean forecast error, Mean Absolute Error, AIC,
                BIC and Durbin Watson test was performed for each fold and the mean was returned back to the caller.
                These performance numbers were used across various orders of AR and MA to determine the performance
                metrics of each of these models.
            </div>
            <h4>Choosing p and q lags for the ARIMA model</h4>
            <div>
                To determine the p-lag and q-lag parameters in the ARIMA model, we use the Autocorrelation Function and
                the Partial Autocorrelation functions whose results are shown in the histograms below. Both imply that a
                p-lag of 1 and a q-lag of 1 are ideal for the ARIMA model. Along with the idenitifed p and q lags, we
                also tried other lag values to see how those ARIMA models performed as well. Looking at the crieria in
                the results table the better performing models are the ARIMA202, ARIMA602 and the ARIMA101.
                Comparatively, the ARIMA101 has the most minimized AIC and BIC while the other two models have a better
                Durbin-Watson value. The forecast error and RMSE values are very close to zero for all three models.
            </div>
            <div style="text-align: center;">
                <img src="./images/auto_correlation.png">
            </div>
            <div>
                The plot below shows the three ARIMA models discussed above, the decomposition model and the baseline
                model compared to the rate of GDP. Our final thoughts are that it does appear that the sentiment score
                predictors did influence the performance of the final models as they seem to not only follow the data
                closely but also have the most optimal values on the criteria we are testing them against. Ultimately,
                we chose the ARIMA101 model as our best fitting model for forecasting economic growth.
            </div>

            <div style="text-align: center;">
                <img src="./images/all_models.png">
            </div>

            <table class="table table-bordered">
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Forecast</th>
                        <th>RMSE</th>
                        <th>AIC</th>
                        <th>BIC</th>
                        <th>ADF</th>
                        <th>Durbin</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Mean</td>
                        <td align=right>0.00612926</td>
                        <td align=right>0.00601676</td>
                        <td>NA</td>
                        <td>NA</td>
                        <td>NA</td>
                        <td>NA</td>
                    </tr>
                    <tr>
                        <td>Linear</td>
                        <td>[0.006935<span style='display:none'>21052642]</span></td>
                        <td align=right>0.00584964</td>
                        <td align=right>-844.715</td>
                        <td align=right>-839.243</td>
                        <td>NA</td>
                        <td align=right>1.24086</td>
                    </tr>
                    <tr>
                        <td>Baseline A<span style='display:none'>R(1)</span></td>
                        <td align=right>0.00351589</td>
                        <td align=right>0.00654511</td>
                        <td align=right>-857.077</td>
                        <td align=right>-848.868</td>
                        <td>NA</td>
                        <td align=right>2.1543</td>
                    </tr>
                    <tr>
                        <td>Moving Av<span style='display:none'>erage
  12</span></td>
                        <td align=right>0.0054909</td>
                        <td align=right>0.00550371</td>
                        <td>NA</td>
                        <td>NA</td>
                        <td align=right>-4.68909</td>
                        <td>NA</td>
                    </tr>
                    <tr>
                        <td>Exp Smoot<span style='display:none'>hing
  12</span></td>
                        <td align=right>0.00481832</td>
                        <td align=right>0.0054752</td>
                        <td>NA</td>
                        <td>NA</td>
                        <td align=right>-5.06463</td>
                        <td>NA</td>
                    </tr>
                    <tr>
                        <td>Decomposi<span style='display:none'>tion</span></td>
                        <td class=xl65 align=right>2.76E-05</td>
                        <td align=right>0.00363306</td>
                        <td>NA</td>
                        <td>NA</td>
                        <td align=right>-7.71544</td>
                        <td>NaN</td>
                    </tr>
                    <tr>
                        <td>AR(2) with <span style='display:none'>sentiments</span></td>
                        <td align=right>0.00609168</td>
                        <td align=right>0.00928314</td>
                        <td align=right>-833.843</td>
                        <td align=right>-822.934</td>
                        <td align=right>-6.53901</td>
                        <td align=right>2.04204</td>
                    </tr>
                    <tr>
                        <td>MA(2) with<span style='display:none'><span
                                style="mso-spacerun:yes">&nbsp;</span>sentiments</span></td>
                        <td align=right>0.00602457</td>
                        <td align=right>0.00991768</td>
                        <td align=right>-841.408</td>
                        <td align=right>-830.499</td>
                        <td align=right>-7.74315</td>
                        <td align=right>1.93423</td>
                    </tr>
                    <tr>
                        <td>ARIMA202</td>
                        <td align=right>-0.000346063</td>
                        <td align=right>0.00680533</td>
                        <td align=right>-744.054</td>
                        <td align=right>-723.15</td>
                        <td>RejectNull</td>
                        <td align=right>2.02152</td>
                    </tr>
                    <tr>
                        <td>ARIMA302</td>
                        <td align=right>0.00445305</td>
                        <td align=right>0.00818253</td>
                        <td align=right>-738.614</td>
                        <td align=right>-715.098</td>
                        <td>RejectNull</td>
                        <td align=right>2.04355</td>
                    </tr>
                    <tr>
                        <td>ARIMA402</td>
                        <td align=right>0.00265413</td>
                        <td align=right>0.00786289</td>
                        <td align=right>-737.749</td>
                        <td align=right>-711.62</td>
                        <td>RejectNull</td>
                        <td align=right>2.01777</td>
                    </tr>
                    <tr>
                        <td>ARIMA602</td>
                        <td align=right>-0.000263907</td>
                        <td align=right>0.0075892</td>
                        <td align=right>-735.929</td>
                        <td align=right>-704.573</td>
                        <td>RejectNull</td>
                        <td align=right>1.97988</td>
                    </tr>
                    <tr>
                        <td>ARIMA400</td>
                        <td align=right>0.00517255</td>
                        <td align=right>0.00855217</td>
                        <td align=right>-739.177</td>
                        <td align=right>-718.274</td>
                        <td>RejectNull</td>
                        <td align=right>2.01598</td>
                    </tr>
                    <tr>
                        <td>ARIMA500</td>
                        <td align=right>0.00463999</td>
                        <td align=right>0.00829884</td>
                        <td align=right>-738.978</td>
                        <td align=right>-715.461</td>
                        <td>RejectNull</td>
                        <td align=right>2.02906</td>
                    </tr>
                    <tr>
                        <td>ARIMA501</td>
                        <td align=right>0.00385933</td>
                        <td align=right>0.00802191</td>
                        <td align=right>-738.256</td>
                        <td align=right>-712.126</td>
                        <td>RejectNull</td>
                        <td align=right>2.03438</td>
                    </tr>
                    <tr>
                        <td>ARIMA402</td>
                        <td align=right>0.00265413</td>
                        <td align=right>0.00786289</td>
                        <td align=right>-737.749</td>
                        <td align=right>-711.62</td>
                        <td>RejectNull</td>
                        <td align=right>2.01777</td>
                    </tr>
                    <tr>
                        <td>ARIMA300</td>
                        <td align=right>0.00548717</td>
                        <td align=right>0.00866605</td>
                        <td align=right>-740.834</td>
                        <td align=right>-722.543</td>
                        <td>RejectNull</td>
                        <td align=right>2.02459</td>
                    </tr>
                    <tr>
                        <td>ARIMA200</td>
                        <td align=right>0.00562409</td>
                        <td align=right>0.00875067</td>
                        <td align=right>-737.74</td>
                        <td align=right>-722.063</td>
                        <td>RejectNull</td>
                        <td align=right>2.0433</td>
                    </tr>
                    <tr>
                        <td>ARIMA100</td>
                        <td align=right>0.00540176</td>
                        <td align=right>0.00849001</td>
                        <td align=right>-738.555</td>
                        <td align=right>-725.491</td>
                        <td>RejectNull</td>
                        <td align=right>2.08597</td>
                    </tr>
                    <tr>
                        <td>ARIMA401</td>
                        <td align=right>0.00462668</td>
                        <td align=right>0.00828668</td>
                        <td align=right>-737.919</td>
                        <td align=right>-714.402</td>
                        <td>RejectNull</td>
                        <td align=right>2.01461</td>
                    </tr>
                    <tr>
                        <td>ARIMA101</td>
                        <td align=right>0.00079489</td>
                        <td align=right>0.0200957</td>
                        <td align=right>-845.649</td>
                        <td align=right>-834.739</td>
                        <td align=right>-6.0942</td>
                        <td align=right>2.13188</td>
                    </tr>
                </tbody>
                </table>
        </div>

        <h3>Prediction</h3>
        <hr/>
        <div>
            New York times business data between Oct 2016 to 12th December 2016 was used to calculate the sentiment
            scores. Once done, ARIMA model (1,0,1) was run on the time series rate difference in order to predict the
            rate difference for 2016Q4. This was then used to calculate the chained GDP value. This value was estimated
            to be 16,668.24 billion dollars.
        </div>

        <h3>Conclusion</h3>
        <hr/>
        <div>
            <p>
                Although our analysis ultimately is using sentiments in business news to forecast economic growth, we
                must make the important distinction that GDP conditions provide important insights to investors and
                businesses. The sentiments reflected should in fact be somewhat representative of the actuality of the
                U.S.’ economic well-being. For example, growth in GDP would lead investors to potentially invest more as
                they expect positive returns while GDP decline or even a recession would lead to the opposite.
                Similarly, businesses like a construction company would for example, look at economic conditions in the
                housing sector to understand whether momentum is improving or slowing and adjust its business strategy
                accordingly.
            </p>

            <p>
                Our generated results took us from using real world data with text mining to implement in time series
                models to show the predictive value business news sentiments have on the U.S. economic growth we have
                from the BEA for the same time period.
            </p>
        </div>

        <h3>Challenges</h3>
        <hr/>
        <div>
        <ul>
            <li>
                NYTimes API call restriction forced us to reduce the data collection to 2 API calls for each day
            </li>
            <li>
                The API has articles not restricted to just the U.S. economy hence, solutions to filter out articles
                from the rest of the world needed to be considered.
            </li>
            <li>
                Because we only have quarterly GDP data, there are only four data points for each year. Our number of
                observations in general is not very large which may not be granular enough to get substantial results
                from the sentiment analysis.
            </li>
        </ul>
        </div>
        <h3>Next Steps</h3>
        <hr/>
        <div>
            <ul>
                <li>
                    With more time, other API’s could be used to collect more articles and add to the value of our
                    sentiment scores.
                </li>
                <li>
                    Since CPI follows the GDP trend closely and is provided on a monthly basis may provide more
                    substantial forecasting results.
                </li>
                <li>
                    Optimizing the parameters for the p and q lags in the ARIMA models could be further explored. The
                    parameters chosen were based off of the autocorrelation function and partial autocorrelation however
                    there are other avenues like the statstools arma_order_select_ic function.
                </li>
            </ul>
        </div>
    </div>

</div> <!-- /container -->

<div class="container hidden content" id="references">
    <h3>References</h3>
    <hr/>
    <div>
        <ol class="list-group">
            <li class="list-group-item">
                Zhang, D., Simoff, S.J. and Debenham, J., 2005, December. Exchange rate modelling using news
                articles and economic data. In Australasian Joint Conference on Artificial Intelligence
                (pp. 467-476). Springer Berlin Heidelberg.
            </li>
            <li class="list-group-item">
                Li, G, Liu, F., 2013, Sentiment analysis based on clustering: a framework in improving accuracy
                and recognizing neutral opinions. Applied Intelligence 40: pp.441–452. Doi: 10.1007/s10489-013-0463-3
            </li>
            <li class="list-group-item">
                Esuli, A. , Sebastiani, F., 2006, SENTIWORDNET: A Publicly Available Lexical Resource for Opinion
                Mining. 5th Conference on Language Resources and Evaluation (LREC’06)
            </li>
            <li>
                "US Department of Commerce, BEA", Bureau of Economic Analysis,
                http://www.bea.gov/national/index.htm#gdp,, 05 Nov 2016
            </li>
            <li>
                "Economic Indicator", Wikipedia, https://en.wikipedia.org/wiki/Economic_indicator, 31 July 2016
            </li>
            <li>
                https://github.com/rouseguy/TimeSeriesAnalysiswithPython/blob/master/time_series/5-Model.ipynb
            </li>
        </ol>
    </div>
</div>  <!-- /container -->

<div class="container hidden content" id="about">
    <div style="margin-top:20px;">
        <h3>Dinesh Malav</h3>
        <a href="https://www.linkedin.com/in/dinesh-malav-9705a7a">Dinesh's LinkedIn Profile</a>
    </div>
    <hr/>
    <div style="margin-top:20px;">
        <h3>Maja Campara</h3>
        <a href="https://ca.linkedin.com/in/maja-campara-22116951">Maja's LinkedIn Profile</a>
    </div>
    <hr/>
    <div style="margin-top:20px;">
        <h3>Naveen Sinha</h3>
        <a href="https://www.linkedin.com/in/naveen-sinha-b0a90b54">Naveen's LinkedIn Profile</a>
    </div>
    <hr/>
    <div style="margin-top:20px;">
        <h3>Udaykiran Tadiparty</h3>
        <a href="https://www.linkedin.com/in/udaykiran-tadiparty-00104a74">Uday's LinkedIn Profile</a>
    </div>
    <hr/>
</div>  <!-- /container -->

<div class="hidden content" id="ipynb">
    <object type="text/html" data="ipynb.html" width="100%" height="800px">
    </object>
</div>  <!-- /container -->


<script>
$(".nav a").on('click',function(e) {
    // make clicked tab active
    $(".nav").find(".active").removeClass("active");
    $(this).parent().addClass("active");

    // remove hidden class from current tab content, hide everything and show this tab contents
    $($(this).attr('href')).removeClass("hidden");
    $(".content").hide();
    $($(this).attr('href')).show();
});
</script>
</body>
</html>
