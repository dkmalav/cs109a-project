<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Forecasting Economic Growth Using Business News Data</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="./bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="./css/style.css" rel="stylesheet">
    <style>
        body {
        padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
        }
    </style>
    <link href="./bootstrap/css/bootstrap-responsive.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
    <script src="../bootstrap/js/html5shiv.js"></script>
    <![endif]-->

    <!-- Fav icons -->
    <link rel="shortcut icon" href="./images/harvard.png">

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <!--<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>

    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./bootstrap/js/bootstrap.min.js"></script>
</head>

<body>


<div class="navbar navbar-inverse navbar-fixed-top">
    <div class="navbar-inner">
        <div class="container">
            <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="brand" href="#">CS109a-Project</a>
            <div class="nav-collapse collapse">
                <ul class="nav">
                    <li class="active"><a href="#home">Home</a></li>
                    <li><a href="#ipynb">Analysis</a></li>
                    <li><a href="#references">References</a></li>
                    <li><a href="#about">About us</a></li>
                </ul>
                <a id="forkme_banner" href="https://github.com/dkmalav/cs109a-project">View on GitHub</a>
            </div><!--/.nav-collapse -->
        </div>
    </div>
</div>

<div id="home" class="container content">
    <!--<div class="hero-unit" style="background: url('http://nite-lite.org/wp-content/uploads/2016/06/business-news.jpg')">-->
    <div class="hero-unit" style="background-color: #2ba6cb; text-align: center;">
        <h1><font color="white">Forecasting Economic Growth Using Business News</font></h1>
        <div><font color="black">Dinesh Malav, Maja Campara, Naveen Sinha, Udaykiran Tadiparty</font></div>
    </div>
    <div>
        <h3>Introduction</h3>
        <hr/>
        <div>
            Business news sentiment can be conjectured to both reflect how the economy is doing, as well as
            influencing the spending of consumers and investors who read it. It is likely, therefore, there is
            predictive value in the sentiment that is contained in business news. This project will collect
            business news over a certain time span, perform sentiment analysis on it, and forecast economic
            growth (GDP). These forecasts are relevant for e.g. investors making investment decisions, or for
            policy makers who wish to meet a certain economic policy objective.
        </div>

        <h3>Objective</h3>
        <hr/>
        <div>
            Predict GDP (economic growth) from the sentiment that is contained in
            <a href="http://www.nytimes.com/">The New York Times</a> business news,
            perhaps supplemented with traditional economic indicators.
        </div>

        <h3>Data Collection</h3>
        <hr/>

        <h4>Economic Data Collection</h4>
        <div>
            To analyze and assess GDP data for the US, quarterly and yearly data were downloaded from the
            <a href="http://www.bea.gov/">U.S. Bureau of Economic Analysis (BEA)</a> dating back to 1947. This data
            contains GDP value in current dollars and chained 2009 dollars in billions. The US GDP data are
            released quarterly. The GDP value is mostly increasing over time except when there is a recession, to measure
            the actual change in the GDP we consider GDP rate of change in chained 2009 dollar value. Following two charts
            show change in GDP and rate of change in GDP value compared to previous quarter. It was observed that rate
            of change does not follow any particular pattern. According to our hypothesis the news sentiment for the
            same period should directly correlate with rate of change in GDP.
        </div>

        <div class="row">
            <div class="span6" style="text-align: center;">
                <img src="./images/usgdp.png">
            </div>
            <div class="span6" style="text-align: center;">
                <img src="./images/usgdp_rate.png">
            </div>
        </div>

        <h4>Business News Collection</h4>
        <div>
            In order to establish correlation between business news and economic growth, it is important to identify
            relevant news and classify them into 'good' or 'bad' news category. This section describes how the business
            news was collected and converted into feature for training machine learning models.

            The New York Times (NYT) provides an Article Search API to fetch data related to news articles. This API
            does not return full text of articles, but it returns a number of helpful metadata such as subject
            terms, abstract, lead paragraph, and date, as well as URLs, which one could conceivably use to scrape the
            full text of articles. For this project only lead paragraph for business articles for specified time period
            were collected. In order to request data from NYT an API key must be requested from
            <a href="http://developer.nytimes.com/">NYT Developers Network</a>. The article search API is limited
            to 1K calls per day, and 5 calls per second. These limits impose challenge for collecting as much data
            as possible for a day and time period. Based on the limited time for the project, the data collection was
            limited to two API calls or 20 article per day. This allowed to request 500 days worth of data per day.
            A python wrapper was developed around NYT Article Search API to get articles for a given start and end date.
            The reqested lead paragraph data are then store in a file with file name reflecting the day in the
            format YYYYMMDD.txt

        </div>
        <div>
            Following code wraps NYT Api in a python class. This code was used to periodically run every day to make
            number of allowed calls to NYT Api. Since API allows only 1000 call per day, the news article collection
            was limited to 2 pages each day or 20 articles for a day.
        </div>
        <pre class="pre-scrollable">
import os
import requests
import time
import json
from datetime import datetime, timedelta


class NYTArticlesApi(object):
    def __init__(self, api_key):
        self.api_key = api_key
        self.url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'

    def get_articles(self, start_date, end_date, page):
        start_date_str = start_date.strftime('%Y%m%d')
        end_date_str = end_date.strftime('%Y%m%d')

        params = {
            'api-key': self.api_key,
            'fq': "section_name:Business",
            'begin_date': start_date_str,
            'end_date': end_date_str,
            'fl': "lead_paragraph",
            'sort': "newest",
            'page': page
        }
        r = requests.get(self.url, params=params)

        if r.status_code == 200:
            response = json.loads(r.text)
            docs = response['response']['docs']
            return [item['lead_paragraph'] for item in docs]

        print "Error: api response: ", r.status_code, "Message :", r.text
        return None

    def fetch_articles(self, start_date, end_date, data_dir):
        cur_date = start_date
        while cur_date >= end_date:
            print 'Processing ', cur_date, '...'
            articles = []
            # get 2 pages for each day, i.e. 20 articles
            for page in range(2):
                articles_page = self.get_articles(cur_date, cur_date, page)
                if articles_page:
                    articles += articles_page
                else:
                    print "No data for page {0}.".format(page)
                    time.sleep(1)
                    break
                time.sleep(1)

            if len(articles) > 0:
                file_name = data_dir + cur_date.strftime('%Y%m%d') + '.txt'
                with open(file_name, 'wb',) as f:
                    for text in articles:
                        if text:
                            f.write(text.encode('utf-8').strip() + '\n')

            else:
                print "Error fetching articles for {0}.".format(cur_date)
                file_name = data_dir + "last_date.txt"
                with open(file_name, 'wb',) as f:
                    f.write(cur_date.strftime('%Y%m%d'))
                return

            cur_date -= timedelta(days=1)
        </pre>

        <div>
            On an average around 7000 articles were collect for a year. It was discovered by reading some articles
            that not all the articles were US news, these articles can be filtered by checking if any of the country
            name other than US was present. It would also allow for testing if rest of the world news also affects the
            economic grown in US.
        </div>

        <div>
            The articles for a each day were saved to individual file but it was difficult to manage data in files. A
            wrapper class was developed for saving data to SQLite database for easier management and quering data as
            required.
        </div>

        <pre class="pre-scrollable">
# Class to load data in a SQLite database
class SqliteDB(object):
    def __init__(self, db_file):
        self.db_file = db_file
        self.con = sqlite.connect(db_file)
        self.cur = self.con.cursor()

    def close_connection(self):
        self.con.close()

    def create_articles_table(self):
        self.cur.execute("CREATE TABLE Articles(Day DATE, ArticleId INT, Article TEXT, UNIQUE(Day, ArticleId) ON CONFLICT REPLACE)")

    def insert_articles(self, file_name, cur_date):
        with open(file_name, 'rb',) as f:
            articles = f.readlines()

        for i, article in enumerate(articles):
            article = article.replace('\'', '')
            self.cur.execute("INSERT INTO Articles VALUES('{0}','{1}','{2}')".format(cur_date, i, article))


    def insert_data(self, data_dir, start_date, end_date):
        print start_date, end_date
        cur_date = start_date
        while cur_date >= end_date:
            cur_date_str = cur_date.strftime('%Y-%m-%d')
            print cur_date_str
            file_name = data_dir + cur_date.strftime('%Y%m%d') + '.txt'
            # print file_name
            if os.path.exists(file_name):
                self.insert_articles(file_name, cur_date_str)

            cur_date -= timedelta(days=1)

        self.con.commit()

    def query_data(self, query):
        df = pd.read_sql(query, self.con)
        return df;
        </pre>

        <div>
            Once data are loaded to database it can be queried to easily group data for any time period.
        </div>
        <pre class="pre-scrollable">
ddb = SqliteDB('../data/articles.sqlite')
query = "SELECT strftime(\"%Y\", Day) as 'year', count(*) as 'count' FROM Articles group by strftime(\"%Y\", Day)"
df = db.query_data(query)

f, axs = plt.subplots(1,1,figsize=(15,5))
plt.title('Number of articles vs year')
plt.ylabel('Number of articles')
df.plot(kind='bar', x='year', y='count', ax=axs, alpha=0.5)
plt.show()
        </pre>

        <div class="row">
            <div class="span12" style="text-align: center;">
                <img src="./images/article_count.png">
            </div>
        </div>

        <h3>Data Exploration</h3>
        <hr/>
        <div>
            One of the important aspects of determining the impact of the business on the GDP was to extract information
            from the words and sentences we were looking at. The way we tried to do that was through sentiment analysis.
            In order to perform sentiment Analysis we used the following approach :
            <ul>
                <li>
                    Since the GDP data was available quarterly, we combined the New York Times data that we received daily to
                    build a corpus for each quarter.
                </li>
                <li>
                    Using the NLTK Sentence Tokenizer, we split the quarterly corpus of into sentences and further into words
                    using the Word Tokenizer
                </li>
                <li>
                    For every word,
                    <ul>
                        <li>
                            We determined the part of speech it belonged to. Only if the word was one of Noun, Adjective, Verb or Adverb
                            was it considered for sentiment scoring
                        </li>
                        <li>
                            Word sense disambiguation was perform on each word to determine the sense of each word in the sentence using
                            NLTK’s less algorithm
                        </li>
                        <li>
                            The output of the above step was a synset composed of the word sense and the part of speech. This synset was
                            then passed to the SentiWord net to get the sentiment score for each word.
                        </li>

                    </ul>
                </li>
                <li>
                    The sentiment score, both positive and negative, received in the above fashion was then aggregated per
                    quarter to determine the overall positive and negative sentiment of the NY Times corpus for that quarter.
                    These sentiment scores then act as exogenous variables when we run the ARIMA model on the endogenous GDP
                    rate.
                </li>
            </ul>
        </div>

        <h3>Data Modeling and Analysis</h3>
        <hr/>
        <div>
            <h4>Assessing the data and criteria for best model selection</h4>

            <div>
                As we are trying to forecast U.S. GDP using business news sentiments, we had determined that this can be
                done using a time series model. This is different from a regular regression problem since: 1) the
                problem is time dependent and 2) the data in most cases experiences some form of trend over time,
                usually seasonality trends.
            </div>
            <div>
                The data analysis performed assesses different models and compares them on six varying crietrion:
                forecast error, root mean square error, AIC, BIC, Dickey-Fuller test and the Durbin-Watson test. Here we
                would like to see the forecast error and root mean square error to be as close to zero, while the AIC
                and BIC should be as minimized as can be. The Dickey-Fuller test does a stationarity check to confirm if
                trends are still in the data and the Durbin-Watson test indicates whether we have auto-correlation. A
                comparison of these results across all the models are in the table below and discussed further below as
                well.
            </div>
            <div>
                The data used as we know is U.S. GDP however, for the modelling we used the rate of change of GDP
                instead. Along with the rate, we also read in the positive and negative scores as the predictors and
                time as a numeric indicator.
            </div>
            <h4>Basic Models: Mean Model, Linear Trend and Baseline (AR(1)) Model</h4>

            <div>
                The first three most basic models implemented were: the Mean Constant Model, Linear Trend Model, and
                Random Walk Model. From these three, the random walk model which is our baseline model outperformed the
                other two. This seems obvious because a mean constant model is a horizontal line at the value of the
                mean rate while a linear regression would not be ideal since it assumes observations are independent
                which is not the case for a time series problem. The chosen baseline model is an auto regressive model
                with a p-lag of one. This is also called a random walk problem where the series is equally likely to go
                up or down in the next period regardless of what it has done in the past. Looking at the criteria
                comparison in the table, the forecast error and the RMSE are close to each other as well as the AIC and
                BIC for the linear trend and baseline model however, the Durbin-Watson test shows that the baseline is a
                better model because its value is close to 2 (very little auto-correlation).
            </div>
            <h4>Stationarity, Smoothing, Decomposition and DIfferencing</h4>

            <div>
                Once the baseline model was established, to implement other time series models we needed the data to be
                stationary. The underlying principle is to model or estimate the trend and seasonality in the series and
                remove those from the series to get a stationary series. Two models were looked at for estimating and
                eliminating trend: a simple moving average model and a simple exponential smoothing model. These models
                used short-term averaging which tends to have the effect of smoothing out the bumps in the original
                series. With this, we wanted to see if an optimal balance between the performance of the mean and random
                walk models could be seen. Based on the six criteria, we were able to derive the forecast error, RMSE
                and Dickey-Fuller which shows that both perform just as well to each other and are stationary however,
                they do not necessarily perform better than the baseline model.
            </div>
            <div>
                We then tried using the approch of differencing and decomposition to see if we can improve the series.
                Decomposition models trend and seasonality and then removes it from the series. The results outputted
                the smallest forecast error and RMSE yet and passed the Dickey-Fuller test for stationarity as well. For
                the remaining three models, first order differencing was used to remove trend along with the sentiment
                score predictors as well which are used in the models.
            </div>
            <h4>Autoregressive and Moving Average Models</h4>

            <div>
                The next two models examined are the auto regressive and moving average models. We wanted to see if we
                can improve from the last models implemented by using a first order difference, inputting the predictors
                and increasing the lag parameter to 2. The auto regressive model seems to outperform the rest so far as
                its Durbin-Watson test is the closest to 2 (no autocorrelation). However, the moving average model had
                the lowest AIC and BIC values while both of these models had slightly higher forecast errors and RMSE
                than the baseline model. The two models perform very closely to each other overall however, our next
                steps involved trying to optimize the lag parameters to assess our final (ARIMA) models and perform
                cross validation.
            </div>
            <h4>Cross Validation</h4>

            <div>
                In order to perform cross validation on the dataset using ARIMA, we had to make sure that we split a
                dataset into test and train in such a way that the training set maintained similar correlation as
                identified on the entire dataset using the ACF and PACF. The AR or MA models did not converge if the
                above condition was not satisfied. Since the ARIMA model highly depends on the correlation between the
                time series data, we could not do a K-FOLD cross validation as there is no guarantee that the time
                periods necessary for prediction would be available in the fold. Hence in order to perform cross
                validation, we split the data such that for example, for predicting 81st period, the first 80 time
                periods were used for training the model and similarly for predicting the 82nd time period, the first 81
                time periods were used and so on. Root mean square error, Mean forecast error, Mean Absolute Error, AIC,
                BIC and Durbin Watson test was performed for each fold and the mean was returned back to the caller.
                These performance numbers were used across various orders of AR and MA to determine the performance
                metrics of each of these models.
            </div>
            <h4>Choosing p and q lags for the ARIMA model</h4>

            <div>
                To determine the p-lag and q-lag parameters in the ARIMA model, we use the Autocorrelation FUnction and
                the
                Partial Autocorrelation functions whose results are shown in the histograms below. Both imply that a
                p-lag
                of 1 and a q-lag of 1 are ideal for the ARIMA model. Looking at the crieria in the results table does
                not
                imply a best performing model. The forecast error and RMSE are the highest, while the AIC and BIC are
                not
                the lowest, and the Durbin-Watson test is a bit higher than that of the auto regressive with sentiments
                model. That being said, the forecast error and RMSE are very close to zero, while the AIC and BIC are
                low
                along with passing the DIckey-FUller test and experiencing very little autocorrelation. The ARIMA model
                is
                still a good fitting model based on these criteria, but it may very well be the case that we could still
                do
                better.
            </div>
            <div style="text-align: center;">
                <img src="./images/auto_correlation.png">
            </div>
            <div>
                The plot below shows all the models plotted with the GDP rate. As we can see, there are some that model
                closely the data. Our final thoughts are that it does appear that the sentiment score predictors did
                influence the performance of the final models as they slightly out performed the baseline model for
                auto correlation.
            </div>

            <div style="text-align: center;">
                <img src="./images/all_models.png">
            </div>

            <table class="table table-bordered">
                <thead>
                <tr>
                    <th>Model</th>
                    <th>Forecast</th>
                    <th>RMSE</th>
                    <th>AIC</th>
                    <th>BIC</th>
                    <th>ADF</th>
                    <th>Durbin</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Mean</td>
                    <td align=right>0.00606656</td>
                    <td align=right>0.00600613</td>
                    <td align=right>NA</td>
                    <td align=right>NA</td>
                    <td align=right>NA</td>
                    <td align=right>NA</td>
                </tr>
                <tr>
                    <td>Linear</td>
                    <td>[0.0068105450092]</span></td>
                    <td align=right>0.00585845</td>
                    <td align=right>-836.931</td>
                    <td align=right>-831.476</td>
                    <td align=right>NA</td>
                    <td align=right>1.23396</td>
                </tr>
                <tr>
                    <td>Baseline AR(1)</span></td>
                    <td align=right>0.00351589</td>
                    <td align=right>0.00653658</td>
                    <td align=right>-850.244</td>
                    <td align=right>-842.061</td>
                    <td align=right>NA</td>
                    <td align=right>2.14702</td>
                </tr>
                <tr>
                    <td>Moving Average 12</td>
                    <td align=right>0.0054909</td>
                    <td align=right>0.00544435</td>
                    <td align=right>NA</td>
                    <td align=right>NA</td>
                    <td align=right>-4.68909</td>
                    <td align=right>NA</td>
                </tr>
                <tr>
                    <td>Exp Smoothing 12</span></td>
                    <td align=right>0.00481766</td>
                    <td align=right>0.00547224</td>
                    <td align=right>NA</td>
                    <td align=right>NA</td>
                    <td align=right>-5.06463</td>
                    <td align=right>NA</td>
                </tr>
                <tr>
                    <td>Decomposition</span></td>
                    <td class=xl65 align=right>-1.72E-06</td>
                    <td align=right>0.0036365</td>
                    <td align=right>NA</td>
                    <td align=right>NA</td>
                    <td align=right>-7.71544</td>
                    <td>NaN</td>
                </tr>
                <tr>
                    <td>AR(2) with sentiments</span></td>
                    <td align=right>0.00609092</td>
                    <td align=right>0.00931215</td>
                    <td align=right>-825.896</td>
                    <td align=right>-815.022</td>
                    <td align=right>-6.53901</td>
                    <td align=right>2.03753</td>
                </tr>
                <tr>
                    <td>MA(2) with sentiments</span></td>
                    <td align=right>0.0060392</td>
                    <td align=right>0.0100374</td>
                    <td align=right>-834.219</td>
                    <td align=right>-823.345</td>
                    <td align=right>-7.74315</td>
                    <td align=right>1.92274</td>
                </tr>
                <tr>
                    <td>ARIMA</td>
                    <td align=right>0.0229957</td>
                    <td align=right>0.0317585</td>
                    <td align=right>-838.2</td>
                    <td align=right>-827.326</td>
                    <td align=right>-6.0942</td>
                    <td align=right>2.1259</td>
                </tr>
                </tbody>
            </table>
        </div>

        <h3>Prediction</h3>
        <hr/>
        <div>
            Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed consectetur mauris vel pellentesque iaculis.
            Quisque sed ante consequat, vehicula nisi id, elementum felis. Aliquam eget euismod sem, vel venenatis nisl.
            Maecenas id neque eget est dignissim tincidunt. In lorem ex, suscipit quis ipsum at, consequat congue ex.
            Nullam ac nunc quam. Maecenas semper finibus dignissim. Mauris imperdiet vel nulla et rhoncus.
            Mauris malesuada fringilla nibh. Ut faucibus tristique dui pellentesque porta. Morbi sem ex, ornare mattis
            dui id, convallis rutrum dolor.
        </div>

        <h3>Conclusion</h3>
        <hr/>
        <div>
            Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed consectetur mauris vel pellentesque iaculis.
            Quisque sed ante consequat, vehicula nisi id, elementum felis. Aliquam eget euismod sem, vel venenatis nisl.
            Maecenas id neque eget est dignissim tincidunt. In lorem ex, suscipit quis ipsum at, consequat congue ex.
            Nullam ac nunc quam. Maecenas semper finibus dignissim. Mauris imperdiet vel nulla et rhoncus.
            Mauris malesuada fringilla nibh. Ut faucibus tristique dui pellentesque porta. Morbi sem ex, ornare mattis
            dui id, convallis rutrum dolor.
        </div>


    </div>

</div> <!-- /container -->

<div class="container hidden content" id="references">
    <h3>References</h3>
    <hr/>
    <div>
        <ol class="list-group">
            <li class="list-group-item">
                Zhang, D., Simoff, S.J. and Debenham, J., 2005, December. Exchange rate modelling using news
                articles and economic data. In Australasian Joint Conference on Artificial Intelligence
                (pp. 467-476). Springer Berlin Heidelberg.
            </li>
            <li class="list-group-item">
                Li, G, Liu, F., 2013, Sentiment analysis based on clustering: a framework in improving accuracy
                and recognizing neutral opinions. Applied Intelligence 40: pp.441–452. Doi: 10.1007/s10489-013-0463-3
            </li>
            <li class="list-group-item">
                Esuli, A. , Sebastiani, F., 2006, SENTIWORDNET: A Publicly Available Lexical Resource for Opinion
                Mining. 5th Conference on Language Resources and Evaluation (LREC’06)
            </li>
            <li>
                "US Department of Commerce, BEA", Bureau of Economic Analysis,
                http://www.bea.gov/national/index.htm#gdp,, 05 Nov 2016
            </li>
            <li>
                "Economic Indicator", Wikipedia, https://en.wikipedia.org/wiki/Economic_indicator, 31 July 2016
            </li>
        </ol>
    </div>
</div>  <!-- /container -->

<div class="container hidden content" id="about">
    <div class="row" style="margin-top:20px;">
        <div class="span2"><img src="http://a.espncdn.com/combiner/i?img=/i/headshots/nfl/players/full/2330.png&w=350&h=254"></div>
        <h3 class="span8">Dinesh Malav</h3>
    </div>
    <hr/>
    <div class="row" style="margin-top:20px;">
        <div class="span2"><img src="http://a.espncdn.com/combiner/i?img=/i/headshots/nfl/players/full/2330.png&w=350&h=254"></div>
        <h3 class="span8">Maja Campara</h3>
    </div>
    <hr/>
    <div class="row" style="margin-top:20px;">
        <div class="span2"><img src="http://a.espncdn.com/combiner/i?img=/i/headshots/nfl/players/full/2330.png&w=350&h=254"></div>
        <h3 class="span8">Naveen Sinha</h3>
    </div>
    <hr/>
    <div class="row" style="margin-top:20px;">
        <div class="span2"><img src="http://a.espncdn.com/combiner/i?img=/i/headshots/nfl/players/full/2330.png&w=350&h=254"></div>
        <h3 class="span8">Udaykiran Tadiparty</h3>
    </div>
    <hr/>
</div>  <!-- /container -->

<div class="hidden content" id="ipynb">
    <object type="text/html" data="ipynb.html" width="100%" height="800px">
    </object>
</div>  <!-- /container -->


<script>
$(".nav a").on('click',function(e) {
    // make clicked tab active
    $(".nav").find(".active").removeClass("active");
    $(this).parent().addClass("active");

    // remove hidden class from current tab content, hide everything and show this tab contents
    $($(this).attr('href')).removeClass("hidden");
    $(".content").hide();
    $($(this).attr('href')).show();
});
</script>
</body>
</html>
